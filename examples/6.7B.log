<run_flexgen>: args.model: facebook/opt-6.7b
model size: 12.386 GB, cache size: 0.562 GB, hidden size (prefill): 0.009 GB
init weight...
start create model 
init all weights 
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
******* OPTLM model init weight
64 
******* OPTLM model init weight
******* OPTLM model init weight
the time init all weights  11.818272829055786
the model construction time  11.949750185012817
   model structure 
InputEmbed
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
SelfAttention
prefill  None
MLP
OutputEmbed

the useful data start from here -------------------------------------
benchmark - generate
args.gen_len  32
input  torch.Size([4, 256])
============ generate loop normal ============
generate start -----
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention prefill--------
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 256, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
------------------------layer name  InputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------************   number of head
self attention decode =======
------------------------layer name  SelfAttention
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  MLP
hidden  TorchTensor(shape=torch.Size([4, 1, 4096]), dtype=torch.float16, device=cuda:0)
------------------------layer name  OutputEmbed
hidden  TorchTensor(shape=torch.Size([4, 1]), dtype=torch.int64, device=cuda:0)
generate stop *******
the model generate time  3.749768018722534
Outputs:
----------------------------------------------------------------------
0: Paris is the capital city of France and the most visited city in the world. It is the most visited city in the world, with over 30 million visitors each year. Paris is the capital
----------------------------------------------------------------------
3: Paris is the capital city of France and the most visited city in the world. It is the most visited city in the world, with over 30 million visitors each year. Paris is the capital
----------------------------------------------------------------------

TorchDevice: cuda:0
  cur_mem: 12.7859 GB,  peak_mem: 13.4502 GB
TorchDevice: cpu
  cur_mem: 0.0000 GB,  peak_mem: 0.0000 GB
model size: 12.386 GB	cache size: 0.562 GB	hidden size (p): 0.009 GB
peak gpu mem: 13.450 GB	projected: False
prefill latency: 2.107 s	prefill throughput: 486.097 token/s
decode latency: 1.632 s	decode throughput: 75.995 token/s
total latency: 3.738 s	total throughput: 34.241 token/s
